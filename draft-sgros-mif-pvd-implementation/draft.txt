



Internet Engineering Task Force                             S. Gros, Ed.
Internet-Draft                                             L. Jelenkovic
Intended status: Informational                                 D. Skvorc
Expires: September 6, 2016                          University of Zagreb
                                                           March 5, 2016


               An implementation of PvD support in Linux
        draft-sgros-an-implementation-of-PvD-support-in-Linux-00

Abstract

   The purpose of this draft is to document two implementations that
   tried to implement PvD architecture document.  One implementation was
   done from scratch (PvD-manager) while another is a part of an
   existing component (NetworkManager).

Status of This Memo

   This Internet-Draft is submitted in full conformance with the
   provisions of BCP 78 and BCP 79.

   Internet-Drafts are working documents of the Internet Engineering
   Task Force (IETF).  Note that other groups may also distribute
   working documents as Internet-Drafts.  The list of current Internet-
   Drafts is at http://datatracker.ietf.org/drafts/current/.

   Internet-Drafts are draft documents valid for a maximum of six months
   and may be updated, replaced, or obsoleted by other documents at any
   time.  It is inappropriate to use Internet-Drafts as reference
   material or to cite them other than as "work in progress."

   This Internet-Draft will expire on September 6, 2016.

Copyright Notice

   Copyright (c) 2016 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents
   (http://trustee.ietf.org/license-info) in effect on the date of
   publication of this document.  Please review these documents
   carefully, as they describe your rights and restrictions with respect
   to this document.  Code Components extracted from this document must
   include Simplified BSD License text as described in Section 4.e of
   the Trust Legal Provisions and are provided without warranty as
   described in the Simplified BSD License.



Gros, et al.            Expires September 6, 2016               [Page 1]

Internet-Draft  An implementation of PvD support in Linux     March 2016


Table of Contents

   1.  Introduction  . . . . . . . . . . . . . . . . . . . . . . . .   2
     1.1.  Requirements Language . . . . . . . . . . . . . . . . . .   3
   2.  Common implementation mechanism . . . . . . . . . . . . . . .   3
   3.  PvD-manager Implementation  . . . . . . . . . . . . . . . . .   3
     3.1.  Architecture  . . . . . . . . . . . . . . . . . . . . . .   4
     3.2.  PvD Properties  . . . . . . . . . . . . . . . . . . . . .   6
     3.3.  Deployment  . . . . . . . . . . . . . . . . . . . . . . .   7
     3.4.  Implementation Details  . . . . . . . . . . . . . . . . .   8
     3.5.  Test Scenarios  . . . . . . . . . . . . . . . . . . . . .   9
     3.6.  Experiences gained  . . . . . . . . . . . . . . . . . . .  10
       3.6.1.  Linux namespaces  . . . . . . . . . . . . . . . . . .  10
   4.  NetworkManager implementation . . . . . . . . . . . . . . . .  11
     4.1.  NetworkManager's Current Behavior . . . . . . . . . . . .  12
     4.2.  The architecture and components . . . . . . . . . . . . .  13
     4.3.  Options to support PvDs in NetworkManager . . . . . . . .  13
       4.3.1.  Using NMSettingsConnection object to store PvD and
               PvD instance  . . . . . . . . . . . . . . . . . . . .  13
       4.3.2.  Treating NMActiveConnection object as PvD instance
               and PvD . . . . . . . . . . . . . . . . . . . . . . .  15
       4.3.3.  Using NMIP4Config and NMIP6Config objects for PvDs
               and PvD instances . . . . . . . . . . . . . . . . . .  16
       4.3.4.  PvD specific objects  . . . . . . . . . . . . . . . .  17
     4.4.  Implementation  . . . . . . . . . . . . . . . . . . . . .  17
       4.4.1.  Network namespace management  . . . . . . . . . . . .  18
       4.4.2.  Experimental PvD Implementation . . . . . . . . . . .  18
     4.5.  Relationship to PvD Architecture  . . . . . . . . . . . .  18
     4.6.  Experience gained and future work . . . . . . . . . . . .  18
   5.  Server component  . . . . . . . . . . . . . . . . . . . . . .  18
   6.  Acknowledgements  . . . . . . . . . . . . . . . . . . . . . .  19
   7.  IANA Considerations . . . . . . . . . . . . . . . . . . . . .  19
   8.  Security Considerations . . . . . . . . . . . . . . . . . . .  19
   9.  References  . . . . . . . . . . . . . . . . . . . . . . . . .  19
     9.1.  Normative References  . . . . . . . . . . . . . . . . . .  19
     9.2.  Informative References  . . . . . . . . . . . . . . . . .  19
     9.3.  Implementation repositories . . . . . . . . . . . . . . .  20
     9.4.  URIs  . . . . . . . . . . . . . . . . . . . . . . . . . .  20
   Authors' Addresses  . . . . . . . . . . . . . . . . . . . . . . .  20

1.  Introduction

   TBD.








Gros, et al.            Expires September 6, 2016               [Page 2]

Internet-Draft  An implementation of PvD support in Linux     March 2016


1.1.  Requirements Language

   The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
   "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
   document are to be interpreted as described in RFC 2119 [RFC2119].

2.  Common implementation mechanism

   In this section we describe some common implementation concepts.
   Basically, they follow from the preparation phase when different
   approaches were analysed.

   First of, the implementations are all done on modern Linux operating
   system, more specifically, Fedora Desktop distribution.  The main
   characteristic of this distribution is that the main IPC mechanism is
   D-Bus.  So, D-Bus is also used by the two implementations described
   in this draft.

   Next, in the analysis phase the implementations assessment was done
   to determine which mechanisms to use with the main goal of having
   control over the use of provisioning domains by applications.  In
   other words, the main issue was how to control which PvD will an
   application use in the presence of multiple provisioning domains.
   The problems we tried to address are well described in RFC 6418
   [RFC6418].  The conclusion was that the network namespaces are the
   most appropriate mechanism to control application use of provisioning
   domains.

   Still, while network namespaces were found to be the most appropriate
   machanism for separation there is a big issue in that it is not
   possible to have the same PvD instance in separate network
   namespaces.  To get around that issue multiple PvD instaces could be
   used by a single node.  Yet, this is not without the problems either.
   First, from the operational perspective this leads to proliferation
   of IP addresses and to issues with accountability.  It is even a
   bigger issue on IPv4 networks that have restricted number of PvD
   instances per each PvD.

3.  PvD-manager Implementation

   PvD-manager is a client-side component for IPv6 network auto-
   configuration based on multiple provisioning domains (PvDs), as
   described in RFC 6418 [RFC6418] and RFC 7556 [RFC7556].  PvD-manager
   is orthogonal to existing system, which means it does not interfere
   with regular network behavior: none of the services and settings used
   by Network Manager and similar components are not changed nor
   affected.  Implementation and documentation for PvD-manager is
   available at its git repository [PvD-manager].



Gros, et al.            Expires September 6, 2016               [Page 3]

Internet-Draft  An implementation of PvD support in Linux     March 2016


3.1.  Architecture

   PvDs are implemented through Linux network namespaces.  For each
   coherent PvD information set received on the network interface, PvD-
   manager creates a separate network namespace and configures received
   network parameters within that namespace.  Since each network
   namespace uses separate IP stack which is isolated from other
   namespaces, potentially conflicting network parameters received from
   different network providers can safely coexist on a single host.
   PvD-manager manages only those newly created namespaces associated
   with the PvDs and their network settings and leaves the default
   network namespace intact.  That way, all the existing network
   management components, such as Network Manager, continue to work
   unobtrusively.

   Figure 1 presents an overview of a system and its components which
   uses PvD-manager.

 +---------------------------------------------------+  +-------------+
 | +-----------------------------------------------+ |  |   +-------+ |
 | | PvD-manager                                   | |  | +-+ radvd | |
 | | +-----------+   +-----------+   +-----------+ | |  | | +-------+ |
 | | | pvdserver +->-+   pvdman  +-<-+ ndpclient | | |  | |           |
 | | +-----+-----+   +-------+---+   +-----+-----+ | |  | | +-------+ |
 | +-------|-----------------|-------------|-------+ |  | +-+ httpd | |
 |         |d-bus      create|configure    |RA/RS    |  | | +-------+ |
 | get_pvds|           delete|             |HTTP     |  | |           |
 | +-------|-------+    +----+---------+   |         |  | |           |
 | | +-----+-----+ |join|   network    |  -+---------+--+-+--        -+-
 | | |  MIF API  +------+  namespace   |             |  |             |
 | | +-----------+ |    |  operations  |             |  |             |
 | |               |    +--------------+             |  |             |
 | |   PvD aware   |                                 |  |             |
 | |  application  |                                 |  |             |
 | +---------------+                    Client (PC)  |  |     Router  |
 +---------------------------------------------------+  +-------------+

               Figure 1: PvD prototype architecture overview

   PvD-manager receives network configurations through Router
   Advertisement (RA) messages.  Modified version of PvD aware radvd
   daemon [radvd] is used.  Each RA may contain one or more network
   configurations which are classified either as explicit or implicit
   PvDs.  Explicit PvD is a set of coherent NDP options explicitly
   labeled with a unique PvD identifier and nested within a special NDP
   option called PVD container, as described in draft-ietf-mif-mpvd-ndp-
   support-02 [I-D.ietf-mif-mpvd-ndp-support].  Multiple explicit PvDs
   may appear in a single RA, each within a different PvD container



Gros, et al.            Expires September 6, 2016               [Page 4]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   option, as long as they are labeled with different PvD identifiers.
   Implicit PvD is just another name for top-level NDP options placed
   outside the PvD container option, as in regular PvD unaware router
   advertisements.  Since implicit PvDs are not labeled with PvD
   identifier, PvD-manager automatically generates an identifier for
   internal use and configures the implicit PvD on the host in the same
   way as if it was the explicit one.  Only one implicit PvD is allowed
   per RA.  In current prototype, UUID is used as a PvD identifier.

   Each PvD, either explicit or implicit, is associated with a network
   namespace with a single virtual network interface (besides the
   loopback) of macvlan type, where PvD-related network parameters are
   configured.  To establish a connectivity to the outside world,
   virtual interface is connected to the physical interface on which the
   related PvD information is received through the RA.  Each virtual
   interface is assigned a link-local IPv6 address (fe80::/64) and one
   or more addresses derived from Prefix Information options if present
   in the received RA.  Besides the IP addresses, PvD-manager configures
   the routing tables and DNS records within the namespace.  By default,
   a link-local and default route via the RA-announcing router are added
   to the routing table, regardless of the routing information received
   in RA.  Additional routing information is configured if Route
   Information options are received in RA.  Finally, for each RDNSS and
   DNSSL option received in RA, PvD-manager creates a record in
   /etc/netns/NETNS_NAME/resolv.conf, where NETNS_NAME is a name of the
   network namespace associated with the PvD.

   PvD aware client application uses PvD API to get a list of available
   PvDs configured on the local host, and activate chosen PvD to use it
   for communication.  Information about configured PvDs are exposed to
   applications by a special PvD service running on local host.  D-Bus
   is used to connect applications to PvD service.  Upon PvD activation,
   client application is switched to the network namespace associated
   with the selected PvD.  Further network operations (socket creation,
   sending and receiving data) are performed within that namespace.
   Once obtained by the application, socket handles are linked to the
   network namespace they were originally obtained from and continue to
   work in that namespace, regardless of whether the application
   switches to another namespace at some time later.  This enables the
   application to use multiple PvDs simultaneously.  The only
   requirement is that the application is running within a proper
   network namespace while obtaining a socket.

   PvD unaware clients operate as before.  Although they are not able to
   use PvD API to select a certain PvD, they can still be forced to use
   a specific PvD by starting them in a network namespace associated
   with that PvD.  To run a program within a given namespace, it should
   be started with:



Gros, et al.            Expires September 6, 2016               [Page 5]

Internet-Draft  An implementation of PvD support in Linux     March 2016


      ip netns exec <pvd-namespace-name> <command with args>

   or they can be started with provided launchers ("pvd_run" and
   "pvd_prop_run").

   As per RFC 7556 [RFC7556], implemented PvD system provides basic,
   intermediate and advanced PvD support (in APIs) for client
   applications.  Only difference is that our basic support doesn't
   provide automatic selection for PvD unaware application - it must be
   started with PvD launcher with manual selection of PvD.  Intermediate
   and advanced PvD support require some additional properties
   (metadata) provided with PvD.  Next section describe used mechanism
   to provide such information to PvD-manager which then provide such
   information to client applications.

3.2.  PvD Properties

   With RA messages routers provides network related parameters for
   PvDs.  Other parameters that can be used to detail properties of
   particular PvD (an application can use them to better select PvD) in
   this draft are called "PvD properties" or just "properties".

   In this prototype implementation, PvD properties are also provided by
   router, but only on request, using HTTP protocol on router's link-
   local address, using port 8080.  Router's link-local address is saved
   by PvD-manager when RA was received.

   Upon receiving PvD information from router, PvD-manager tries to get
   a file with PvD properties from the same router.  If such file
   exists, network related PvD parameters are extended with ones
   (properties) from received file.

   Client application receives all those additional properties from PvD-
   manager, and may select appropriate PvD based on them.

   Current implementation is very rudimentary: files on router are in
   JSON format.  PvD-manager interpret them - create a dictionary of
   them, but only because PvD-manager it's written in Python and using
   JSON is easy, while client applications are written in C.  In real
   implementation this should be reversed - only client should interpret
   file with PvDs' properties.

   Properties used in this prototype are just an example ("name",
   "type", "bandwidth", "price"), not to be used in some protocol
   specification.  We presented one mechanism to provide additional PvD
   properties obtained by some mechanism (not by RAs) and let client
   application decide what to do with them.




Gros, et al.            Expires September 6, 2016               [Page 6]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   Figure 2 presents example properties for several PvDs when obtained
   from two routers (R1 and R2 from test scenarios described in
   Section 3.5.

   From R1:
   [
       {
           "name": "Home internet access",
           "type": ["internet", "wired"],
           "id": "implicit",
           "bandwidth": "10 Mbps",
           "pricing": "free"
       },
       {
           "name": "TV",
           "type": ["iptv", "wired"],
           "id": "f037ea62-ee4f-44e4-825c-16f2f5cc9b3e",
           "bandwidth": "10 Mbps",
           "pricing": "free"
       }
   ]
   From R2:
   [
       {
           "name": "Cellular internet access",
           "type": ["internet", "cellular"],
           "id": "implicit",
           "bandwidth": "1 Mbps",
           "pricing": "0,01 $/MB"
       },
       {
           "name": "Phone",
           "type": ["voice", "cellular"],
           "id": "f037ea62-ee4f-44e4-825c-16f2f5cc9b3f",
           "bandwidth": "0,1 Mbps",
           "pricing": "0,01 $/MB"
       }
   ]

                      Figure 2: PvD property examples

3.3.  Deployment

   PvD architecture assumes presence of at least one router which runs
   modified version of radvd daemon [radvd], described in Section 5.
   Through RA messages, router conveys network related parameters to
   client host (prefix, routes, DNS servers and domains).  Router should




Gros, et al.            Expires September 6, 2016               [Page 7]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   also provide PvD properties, using an HTTP server on port 8080
   attached to router's link-local IP address.

   DNS servers aren't part of PvD architecture but could be used to
   demonstrate that different PvDs can use different DNS servers.

   PvD-manager is a daemon for client host.  Currently PvD-manager
   consists of several modules.  Main module maintain PvD information,
   creates, updates and deletes namespaces.  NDP module listens for RA
   messages, parse them and forward them to main module.  API server
   module listens for client application requests (using d-bus) and
   responds to them, and also send signals to clients when a change
   occurred in PvDs.

   Before starting a network connection, PvD aware client application
   should first request PvDs from PvD-manager.  Next, one PvD should be
   selected (activated) and only then network connection(s) created.  If
   other PvDs are required (later or in parallel), same procedure must
   be followed: select PvD first and then create connections.
   Connection will continue to operate within PvD in which was created,
   regardless of PvDs selected later.

   PvD unaware application should be started with PvD launcher to use
   certain PvD.  Otherwise, such application will behave as PvDs weren't
   present ("as usual").

3.4.  Implementation Details

   Proposed PvD architecture is based on Linux namespaces as PvD
   isolation mechanism.  Isolation namespaces provide resolve many
   issues about overlapping and conflicting network parameters for
   different PvDs.  However, they also impose some requirements that may
   limit usage in certain environment, especially ones based on public
   IPv4 addresses.  One of the main problem with namespaces is that each
   namespace requires an IP address (since namespace emulates network
   from link-layer and up).

   Only IPv6 is used in implementation.  Main reasons include using RA
   messages as PvD information provider and unrestricted generation of
   IPv6 address per PvD.

   A library is created with interfaces (API) for PvD operations,
   currently only for programs in C programming language.  For
   communication between client application (using provided API) and
   PvD-manager d-bus service is used.  Implemented interface include PvD
   retrieval methods (pvd_get_by_id, pvd_get_by_properties), PvD
   selection (pvd_activate) and registration for events when PvDs change




Gros, et al.            Expires September 6, 2016               [Page 8]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   (pvd_register_signal).  Sample test applications which demonstrate
   API usage and PvD system possibilities are provided in repository.

   PvD-manager is implemented in Python 3 because it allows rapid
   prototyping using network managing modules (mainly pyroute2 and
   netaddr).

   More details about implementation of PvD-manager is available in its
   documentation [PvD-manager].

3.5.  Test Scenarios

   Test scenarios used for validating implementations include a system
   with one client host, two routers and two hosts that act as servers,
   as presented on Figure 3.  All hosts are running as virtual machines.

               fd01::1/64+----+                                   +----+
         2001:db8:1::1/64|    |2001:db8:10::1/32 2001:db8:10::2/32|    |
            +----------o-+ R1 +-o-------------------------------o-+ S1 |
 +------+   |            |    |        :     [VMnet3]             |    |
 |      |   |            +----+        :                          +----+
 |Client+-o-+ [VMnet2]                 : (for some tests this is linked)
 |      |   |            +----+        :                          +----+
 +------+   |            |    |        :     [VMnet4]             |    |
            +----------o-+ R2 +-o-------------------------------o-+ S2 |
         2001:db8:2::1/64|    |2001:db8:20::1/32 2001:db8:20::2/32|    |
               fd02::1/64+----+                                   +----+

          Figure 3: Network configuration used in test scenarios

   All server hosts, including routers have configured HTTP and DNS
   servers providing many possibilities for testing.  Routers, in RAs
   advertise prefixes as shown on Figure 3.  Local addresses are used in
   explicit PvDs (simulating some specific service), while public in
   implicit.

   Example network configurations from RFC 7556 [RFC7556] are simulated
   with Figure 3 with various applications on Client and servers S1 and
   S2.  S1 is accessible by client only through implicit PvD provided by
   R1, while S2 similarly, only over PvD provided by R2.

   If S1 simulate one service, and S2 another, client application can
   select PvD based on service required and connect to S1 or S2.  Or a
   PvD aware application can use both in parallel.

   VPN was simulated by a tunnel between Client and S2, created within
   implicit PvD provided by R2.  Then tunnel was added as another PvD.
   S2 for this scenario had local address (and prefix) as local



Gros, et al.            Expires September 6, 2016               [Page 9]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   addresses on R2 network: there were two PvDs with same prefix on
   Client.  However, client applications running in those two different
   PvDs for the same IP address (fd02::1) connected with different
   servers: one with R2, and another (which used "VPN" PvD) with S2.

   In some scenarios both S1 and S2 were connected to both routers R1
   and R2.  In this scenarios better PvD could be chosen for connecting
   with servers, if properties for PvDs are provided.  Also, when some
   connection fails - when some PvD loses connectivity, application can
   reset connections, refresh PvD availability from PvD-manager and
   chose again among active PvDs.

   More details for described scenarios (and some other) are provided in
   demonstration test cases [PvD-manager].

3.6.  Experiences gained

   Present some experience gained from the implementation.  What do you
   think was good?  What would you do differently if you were to
   implement this again.  What about language/environment, was it good,
   what was bad?

3.6.1.  Linux namespaces

   Using Linux namespaces still seems best option for PvD realization
   despite its drawbacks.  Its isolation and ease of use from PvD-
   manager and client application perspective can hardly be outmatched
   with other solutions.  Besides already mentioned need for separate IP
   address per namespace, there are several more issues with namespaces.

   Managing namespaces (creation, deletion, modification) requires root
   privileges (as expected).  However, even switching an application
   from one namespace to another is possible only if application has
   root privileges.  This currently limits this namespace approach to
   only applications run by root.  For lifting this limitation, changes
   in Linux kernel and namespace handling is required.  Some sort of
   permission system should also be applied to namespaces (e.g.
   similarly with permissions on files and system objects).

   Switching namespace from within the application with setns doesn't
   update DNS related configuration as expected.  When an application is
   started with commands "ip netns exec <namespace-name> <application>
   [arguments]", DNS configuration is updated (/etc/resolf.conf is the
   one from /etc/netns/<namespace-name>/).  However, setns doesn't
   replicate that behavior, and that manipulations should be done
   separately (mounting certain directories/files).





Gros, et al.            Expires September 6, 2016              [Page 10]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   When a namespace is created, a virtual device is created that is
   linked with physical device (and it gets assigned its own MAC
   address).  However, if on particular physical device can't be linked
   virtual one (like VPN), then either physical device must be moved to
   certain PvD or some sort of bridge created and devices attached to
   that bridge that can be moved to namespace.  Sometimes, it's better
   to move physical device to particular namespace (PvD) and allow only
   some applications its usage (e.g. like VPN).

   Managing namespace: creating, deleting, adding device to it, adding
   ip address and routes are operations performed within PvD-manager and
   they aren't instant.  Maybe that is expected and "normal" since such
   operations aren't to be performed frequently (only when something in
   network changes).  However, when testing frequent changes in PvDs
   (routers were connected and disconnected) significant delay in PvD
   aware application was detected.  Sometimes PvD-manager's API server
   module (that is responsible for client communication) become
   unresponsive for at least several seconds.  This could be a bug in
   PvD-manager or result of changes PvD-manager sent being applied by
   Linux kernel.

   Recently added feature of Linux kernel (January, 2016), named Virtual
   Routing and Forwarding (VRF) [1], seems possible alternative to
   namespaces.  However, it should be investigate further to create some
   conclusions.

4.  NetworkManager implementation

   NetworkManager is a software component used in modern Linux
   distributions to control network connections.  It runs as a daemon
   tracking and reacting to the network related events, either those
   coming from the network (like, for example, Router Advertisements) or
   those from the local system (e.g. attachment of a new networking
   device).  Furthermore, it exposes certain methods and properties over
   D-Bus interface so that it can be controlled by different clients,
   the most prominent being network manager applet and nmcli command
   line tool.

   Due to its importance in the modern Linux distributions it was
   valuable experience to try to implement PvDs within it.  Yet,
   NetworkManager is a very complex piece of a software that wasn't
   designed with PvDs in mind so it wasn't straightforward task to try
   to implement them and there were some difficulties that had to be
   overcome.  In the following text we'll first describe how
   NetworkManager behaves now with respect to multiple PvDs, then we'll
   describe one approach on how to add support for multiple PvDs.





Gros, et al.            Expires September 6, 2016              [Page 11]

Internet-Draft  An implementation of PvD support in Linux     March 2016


4.1.  NetworkManager's Current Behavior

   In this section we describe how unmodified NetworkManager behaves
   with respect to multiple provisioning domains.

   First, we have to state that NetworkManager doesn't use network
   namespaces, that is, everything is kept in one network namespace that
   we'll call "root network namespace" or "main network namespace".  So,
   all devices and all the configuration parameters are in one place.

   The situations in which NetworkManager handles multiple provisioning
   domains are:

   VPN Connections.
         When user activates VPN connection upon successful
         establishment of a connection, NetworkManager receives PvD
         instance for the given VPN connection.  In some cases after
         establishing VPN there is virtual device present (e.g.
         OpenVPN), while in other cases there are no new devices but
         only appropriate additions to the existing IP packet processing
         path (e.g.  IPsec).  In any case, interface and the associated
         PvD instance are assigned and present in the root network
         namespace.  Furthermore, DNS data is also merged with existing
         data.  It isn't hard to see that this mixing of PvD instances
         might lead to very complex situations in which it is hard to
         control which PvD instance will be used.  One possibility to
         control what will be accessible is default route.  Namely, it
         is possible to instruct NetworkManager not to install default
         route via VPN PvD instance even though it was sent by VPN
         gateway.

   Concurrent active wired and wireless connections.
         Again, like in the previous case, all the settings are mixed
         and this mixing has a consequence of not be able to use the two
         connections in parallel.  Current policy that is enforced by
         NetworkManager is to prefer wired connection for default route.
         In other words, unless there are more specific routes that use
         wireless connection, it will be not used while wired network
         connection is used.

   Multiple IPv6 routers on a local network.
         This is interesting use case even though it is not as common
         now.  Namely, there is a possibility that there are two IPv6
         capable routers on the local network that overlap in
         connectivity.  What NetworkManager will do in this case is that
         all PvDs received in RAs will be merged together per device.

   Multiple concurrent DNS servers on a local network.



Gros, et al.            Expires September 6, 2016              [Page 12]

Internet-Draft  An implementation of PvD support in Linux     March 2016


         This is a final use case that isn't possible by the current
         protocol design.  Namely, DHCP offers received are treated as
         alternative and there is no way to have multiple DNS servers on
         a single local network.

4.2.  The architecture and components

   We can describe NetworkManager in terms of static and runtime
   architecture.  The static architecture is reflected by the source
   code organization.  For the purpose of this draft will only present
   NetworkManager itself, but take into account that there are
   additional components, too.  So, the NetworkManager's code is in src/
   directory of top-level NetworkManager directory.  There you'll find
   the following subdirectories:

   devices/
         Objects that control devices.

   platform/
         Platform specific code that isolates NetworkManager from
         specifics of a certain operating system.  The only supported
         platform at the time this was written in Linux.

   rdisc/
         Objects that are used to send/receive RA/RS messages.  There is
         a platform independent object and platform dependent objects.
         Platform dependent objects do real receive/send events.

   settings/


   vpn-manager/


4.3.  Options to support PvDs in NetworkManager

   As always, the same goal can be achieved in multiple ways, so here
   are the options on how PvDs can be implemented within NM.  Basically,
   there are two main approaches: first, existing objects can be
   enhanced so that they can represent PvDs or a completely new object
   can be introduced.

4.3.1.  Using NMSettingsConnection object to store PvD and PvD instance

   Each network connection (which is not the same as PvD or PvD
   instance) is stored in NMSettingsConnection object.  Those objects
   are generated from static files or dynamically during




Gros, et al.            Expires September 6, 2016              [Page 13]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   NetworkManager's execution.  NMSettingsConnection objects are
   initialized from the following sources:

   Distribution configuration files.  System dependent network
   configuration files (e.g. /etc/sysconfig/network-scripts for RHEL
   based systems) are read by NM via plugins and NMSettingsConnection
   objects are created as a result.

   Network manager specific configuration.  NetworkManager has its own
   configuration files that are stored in /etc/NetworkManager/system-
   connections/.

   Dynamically created configurations.  While running, NetworkManager
   allows new configurations to be created via D-Bus interface.

   Note that NetworkManager has a concept of profiles that are used in
   the case of Wired networks.  Basically, those are settings which are
   not bound to any specific network interface.  Profiles can have
   802.1x type of credentials assigned to them.

   So, the idea of integrating PvDs into NetworkManager is for each new
   PvD or PvD instance to create a new NMSettingsConnection object.  The
   modification to NMSettingsConnection should be extended with PvD ID
   parameter.

   There are several potential problems with this approach:

      There is a difference between NMSettingsConnection on the one
      hand, and PvD and PvD instance on the other hand.  For example,
      some NMSettingsConnection defines a network connection that should
      be configured using DHCP and in that case the NMSettingsConnection
      isn't PvD nor PvD instance.  On the other hand,
      NMSettingsConnection can be the same as PvD instance.  This is the
      case with static IPv4 configurations when a user specifies
      concrete IP addresses.  Finally, NMSettingsConnection can be PvD
      only in the case of IPv6 when host part is generated from MAC
      address.

      When PvDs and PvD instances are received they are valid only for
      the interface on which they are received.  But, a user can request
      any NMSettingsConnection object to be activated on any interface
      which isn't possible.

      Also, this can create confusion.  Take for example preconfigured
      NMSettingsConnection which is now treated as PvD with a specific
      PvD ID, and it is defined to use DHCP for the configuration.
      Obviously, this PvD ID is expected to be valid on a certain
      interface on a specific attachment point.  But due to the way the



Gros, et al.            Expires September 6, 2016              [Page 14]

Internet-Draft  An implementation of PvD support in Linux     March 2016


      interface is configured (DHCP) it can actually be activated on any
      interface on any network that supports DHCP.  Thus, it might
      easily happen that a user by mistake activated this particular
      NMSettingsConnection on a "wrong" network and so makes a user
      believe the network is active while in the reality it is not.

      Note that even NMSettingsConnection objects that contain
      credential information aren't guaranteed to retrieve the same PvD
      every time the connection is made.  Namely, there are AAA servers
      and infrastructure that allow clients with a same credentials to
      connect to multiple networks, and thus to potentially receive
      multiple PvDs.

      Finally, the problem is that on a single network interface only
      one NMSettingsObject might be activated and so this prevents
      having multiple PvDs on a single interface.

   Those problems are not unsolvable, i.e. they could be solved by
   modifying certain aspects of the NetworkManager in general, and
   NMSettingsObject in particular.

4.3.2.  Treating NMActiveConnection object as PvD instance and PvD

   Whenever a connection is made in NetworkManager an object is created.
   Basically, there are two classes for the object, both of which
   inherit from NMActiveConnection base class.  Which class is used
   depends on the type of the connection.  Basically, the only
   distinction is made between VPN connections that are represented by
   NMVPNConnection objects and other connections that are represented by
   NMActRequest objects.  The main task of NMActiveConnection is to bind
   NMSettingsConnection with NMDevice objects.

   The idea in this case is to treat NMActiveConnection as a PvD or a
   PvD instance, i.e. on each new PvD or PvD instance received new
   NMActiveConnection is created.

   But, there are still some problems:

      Since NMActiveConnection objects are transient that means that
      there would be no history of PvDs used.  This might, or might not
      be a problem, depending on whether we need this history or not.

      The cases when the history would be necessary is if we cache some
      information for the next time we connect to the given PvD.  The
      second case is if there are processes still using PvD through API
      and thus the information about PvD must live until the process
      dies.  Note that this letter problem could be solved with delayed
      removal of NMActiveConnections or by some asynchronous mechanism



Gros, et al.            Expires September 6, 2016              [Page 15]

Internet-Draft  An implementation of PvD support in Linux     March 2016


      informing applications that specific NMActiveConnection isn't
      available any more.

      The second problem is the question if there could exist two
      ActiveConnection objects that were created from the same
      NMSettingsConnection object, i.e. can NMSettingsConnections be
      shared.

      The third problem is that it will happen from within a single
      NMActiveConnection that two or more PvDs are received and this
      requires that NMActiveConnection is a factory for itself.

4.3.3.  Using NMIP4Config and NMIP6Config objects for PvDs and PvD
        instances

   NetworkManager has object/classes for storing IPv4 (libnm-core/nm-
   setting-ip4-config.c) and IPv6 (libnm-core/nm-setting-ip6-config.c)
   settings.  More precisely, those objects are used to expose network
   settings of devices to the rest of the NetworkManager.  So, in some
   way they are PvDs in a sense that each of them contains enough
   information to allow connection to the network.

   The problem is that internally NetworkManager keeps a single IPv4/
   IPv6 configuration object per device and in addition it merges all
   received configuration data on a single interface.

   Specifically, in case of configuration data received in RAs
   everything is kept in the object NMRdisc defined in src/rdisc/nm-
   rdisc.h.  There you'll find arrays of received configuration data.
   NetworkManager assumes that a single router sends all the
   configuration data.  This assumption is not valid on a multihomed
   network, or a network that can send multiple provisioning domains
   within each RA.  What would be necessary is to change this structure
   so that configuration data is kept separate for each router and
   provisioning domain.

   The problems in this case are:

      NMIPxConfig objects were not intended to keep information about
      available IPv4 and IPv6 addresses but to make available addresses
      configured on device.  So, it reverses the purpose of those
      objects which isn't accepted so well.

      Again, those are transient objects and thus there is no history.
      It is possible to keep every object alive, but NM isn't designed
      to behave in such way.





Gros, et al.            Expires September 6, 2016              [Page 16]

Internet-Draft  An implementation of PvD support in Linux     March 2016


      It seems that in libnm there is no way to obtain a list of IPv4
      and IPv6 objects.

4.3.4.  PvD specific objects

   This is the final alternative and the most intrusive one.  The idea
   is that settings, active connections and IPv6 and IPv6 objects/
   classes stay as is, but instead, when each new connection is
   established a new PvD data structure is created.  PvD is inferred
   from configuration settings or the NetworkManager received explicit
   PvD.

   This would solve the problem that some settings might be used to
   obtain different PvDs which isn't known until connection is
   established.  For example, if we are using DHCP to configure the
   interface, then, PvD received depends on the PoA.

   It would also solve the problem that the user might try to
   instantiate one PvD, while some other is actually in use.  This way,
   after the connection is established, appropriate PvD is searched for,
   or new one is created.

   This is most intrusive change that would require change in APIs and
   thus break compatibility with the existing applications (or require a
   completely new API).

4.4.  Implementation

   To understand NetworkManager it is first necessary to understand
   GObject system which is created so that it is possible to use object
   oriented programming in the C programming language, and also to allow
   easy integration with D-Bus.

   The implementation of PvD support within NetworkManager was done with
   the following requirements in mind:

      For backwards compatibility there should be a root network
      namespace which is handled by NetworkManager as before, i.e.  it
      contains all configurations received merged into one.

      NetworkManager doesn't touch network namespaces created by other
      applications, like for example different virtualization solutions.

   The main objects that NetworkManager is based on are devices,
   connections and properties.






Gros, et al.            Expires September 6, 2016              [Page 17]

Internet-Draft  An implementation of PvD support in Linux     March 2016


4.4.1.  Network namespace management

   To support network namespace management it was necessary to add two
   objects: NMNetworkNamespaceController which is a singleton object
   that is used to enumerate available network namespaces, create a new
   one or remove existing network namespace.

   The second object introduced to allow NetworkManager support network
   namespaces is NMNetns.  For each network namespace created by
   NetworkManager one NMNetns object is created.  This object has an
   interface exposed over D-Bus that allows one to query all available
   devices within the network namespace, to take device from some other
   network namespace and to activate connections.

4.4.2.  Experimental PvD Implementation

   The first implementation of PvDs was done using NMIP6Config as a PvD
   container.  Before describing the implementation we have to state
   that the only mechanism currently able to carry PvDs is RA messages.
   NMIP6Config objects are extended with PvD ID field.  At first, there
   was support for different types of PvD IDs and the first implemented
   type was UUID stored in ASCII format.  Later in the development
   process PvD ID types were removed and the only possible type is UUID.
   It seems that this doesn't make implementations less flexible and in
   the same time substantially reduces complexity.

   When RA is received, and after it is processed as usual, a new
   implicit PvD is created from data in RA.  If there are two or more
   routers on the local network, each sending its own configuration
   data, then a separate PvD is created for each RA.  Also, in case
   there are PVD container option in RA it is parsed and additional PvD
   is created from that data.

   This information is then handled to NMDevice object which merges data
   from implicit PvDs (as it does in the unmodified version) but now
   there is also a hash table with set of PvDs received on the given
   interface.  This information is then exposed through
   NMActiveConnection object.

4.5.  Relationship to PvD Architecture

4.6.  Experience gained and future work

5.  Server component

   Write about changes in the radvd component.





Gros, et al.            Expires September 6, 2016              [Page 18]

Internet-Draft  An implementation of PvD support in Linux     March 2016


6.  Acknowledgements

   TBW.

7.  IANA Considerations

   This memo includes no request to IANA.

   All drafts are required to have an IANA considerations section (see
   the update of RFC 2434 [I-D.narten-iana-considerations-rfc2434bis]
   for a guide).  If the draft does not require IANA to do anything, the
   section contains an explicit statement that this is the case (as
   above).  If there are no requirements for IANA, the section will be
   removed during conversion into an RFC by the RFC Editor.

8.  Security Considerations

   All drafts are required to have a security considerations section.
   See RFC 3552 [RFC3552] for a guide.

9.  References

9.1.  Normative References

   [RFC2119]  Bradner, S., "Key words for use in RFCs to Indicate
              Requirement Levels", BCP 14, RFC 2119,
              DOI 10.17487/RFC2119, March 1997,
              <http://www.rfc-editor.org/info/rfc2119>.

   [RFC6418]  Blanchet, M. and P. Seite, "Multiple Interfaces and
              Provisioning Domains Problem Statement", RFC 6418,
              DOI 10.17487/RFC6418, November 2011,
              <http://www.rfc-editor.org/info/rfc6418>.

   [RFC6419]  Wasserman, M. and P. Seite, "Current Practices for
              Multiple-Interface Hosts", RFC 6419, DOI 10.17487/RFC6419,
              November 2011, <http://www.rfc-editor.org/info/rfc6419>.

   [RFC7556]  Anipko, D., Ed., "Multiple Provisioning Domain
              Architecture", RFC 7556, DOI 10.17487/RFC7556, June 2015,
              <http://www.rfc-editor.org/info/rfc7556>.

9.2.  Informative References

   [I-D.ietf-mif-mpvd-dhcp-support]
              Krishnan, S., Korhonen, J., and S. Bhandari, "Support for
              multiple provisioning domains in DHCPv6", draft-ietf-mif-
              mpvd-dhcp-support-02 (work in progress), October 2015.



Gros, et al.            Expires September 6, 2016              [Page 19]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   [I-D.ietf-mif-mpvd-ndp-support]
              Korhonen, J., Krishnan, S., and S. Gundavelli, "Support
              for multiple provisioning domains in IPv6 Neighbor
              Discovery Protocol", draft-ietf-mif-mpvd-ndp-support-03
              (work in progress), February 2016.

   [I-D.narten-iana-considerations-rfc2434bis]
              Narten, T. and H. Alvestrand, "Guidelines for Writing an
              IANA Considerations Section in RFCs", draft-narten-iana-
              considerations-rfc2434bis-09 (work in progress), March
              2008.

   [RFC2629]  Rose, M., "Writing I-Ds and RFCs using XML", RFC 2629,
              DOI 10.17487/RFC2629, June 1999,
              <http://www.rfc-editor.org/info/rfc2629>.

   [RFC3552]  Rescorla, E. and B. Korver, "Guidelines for Writing RFC
              Text on Security Considerations", BCP 72, RFC 3552,
              DOI 10.17487/RFC3552, July 2003,
              <http://www.rfc-editor.org/info/rfc3552>.

9.3.  Implementation repositories

   [PvD-manager]
              Jelenkovic, L. and D. Skvorc, "PvD-manager repository",
              March 2016, <https://github.com/l30nard0/mif>.

   [radvd]    Skvorc, D., "PvD customized radvd daemon", February 2016,
              <https://github.com/dskvorc/mif-radvd>.

9.4.  URIs

   [1] https://www.kernel.org/doc/Documentation/networking/vrf.txt

Authors' Addresses

   Stjepan Gros (editor)
   University of Zagreb
   Unska 3
   Zagreb  10000
   HR

   Email: stjepan.gros@fer.hr








Gros, et al.            Expires September 6, 2016              [Page 20]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   Leonardo Jelenkovic
   University of Zagreb
   Unska 3
   Zagreb  10000
   HR

   Email: leonardo.jelenkovic@fer.hr


   Dejan Skvorc
   University of Zagreb
   Unska 3
   Zagreb  10000
   HR

   Email: dejan.skvorc@fer.hr



































Gros, et al.            Expires September 6, 2016              [Page 21]
