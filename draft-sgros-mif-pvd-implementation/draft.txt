



Internet Engineering Task Force                             S. Gros, Ed.
Internet-Draft                                             L. Jelenkovic
Intended status: Informational                                 D. Skvorc
Expires: September 6, 2016                          University of Zagreb
                                                           March 5, 2016


               An implementation of PvD support in Linux
        draft-sgros-an-implementation-of-PvD-support-in-Linux-00

Abstract

   The purpose of this draft is to document two implementations that
   tried to implement PvD architecture document.  One implementation was
   done from scratch (PvD-manager) while another is a part of an
   existing component (NetworkManager).

Status of This Memo

   This Internet-Draft is submitted in full conformance with the
   provisions of BCP 78 and BCP 79.

   Internet-Drafts are working documents of the Internet Engineering
   Task Force (IETF).  Note that other groups may also distribute
   working documents as Internet-Drafts.  The list of current Internet-
   Drafts is at http://datatracker.ietf.org/drafts/current/.

   Internet-Drafts are draft documents valid for a maximum of six months
   and may be updated, replaced, or obsoleted by other documents at any
   time.  It is inappropriate to use Internet-Drafts as reference
   material or to cite them other than as "work in progress."

   This Internet-Draft will expire on September 6, 2016.

Copyright Notice

   Copyright (c) 2016 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents
   (http://trustee.ietf.org/license-info) in effect on the date of
   publication of this document.  Please review these documents
   carefully, as they describe your rights and restrictions with respect
   to this document.  Code Components extracted from this document must
   include Simplified BSD License text as described in Section 4.e of
   the Trust Legal Provisions and are provided without warranty as
   described in the Simplified BSD License.



Gros, et al.            Expires September 6, 2016               [Page 1]

Internet-Draft  An implementation of PvD support in Linux     March 2016


Table of Contents

   1.  Introduction  . . . . . . . . . . . . . . . . . . . . . . . .   2
     1.1.  Requirements Language . . . . . . . . . . . . . . . . . .   2
   2.  Common implementation mechanism . . . . . . . . . . . . . . .   2
   3.  PvD-manager Implementation  . . . . . . . . . . . . . . . . .   3
     3.1.  Architecture  . . . . . . . . . . . . . . . . . . . . . .   3
     3.2.  PvD Properties  . . . . . . . . . . . . . . . . . . . . .   6
     3.3.  Deployment  . . . . . . . . . . . . . . . . . . . . . . .   7
     3.4.  Implementation Details  . . . . . . . . . . . . . . . . .   8
     3.5.  Test Scenarios  . . . . . . . . . . . . . . . . . . . . .   9
     3.6.  Experiences gained  . . . . . . . . . . . . . . . . . . .  10
       3.6.1.  Linux namespaces  . . . . . . . . . . . . . . . . . .  10
   4.  NetworkManager implementation . . . . . . . . . . . . . . . .  11
     4.1.  NetworkManager's Current Behavior . . . . . . . . . . . .  11
     4.2.  The architecture and components . . . . . . . . . . . . .  12
     4.3.  Implementation  . . . . . . . . . . . . . . . . . . . . .  13
       4.3.1.  Network namespace management implementation . . . . .  14
       4.3.2.  Provisioning domain support implementation  . . . . .  14
   5.  Server component  . . . . . . . . . . . . . . . . . . . . . .  14
   6.  Acknowledgements  . . . . . . . . . . . . . . . . . . . . . .  14
   7.  IANA Considerations . . . . . . . . . . . . . . . . . . . . .  14
   8.  Security Considerations . . . . . . . . . . . . . . . . . . .  15
   9.  References  . . . . . . . . . . . . . . . . . . . . . . . . .  15
     9.1.  Normative References  . . . . . . . . . . . . . . . . . .  15
     9.2.  Informative References  . . . . . . . . . . . . . . . . .  15
     9.3.  Implementation repositories . . . . . . . . . . . . . . .  16
     9.4.  URIs  . . . . . . . . . . . . . . . . . . . . . . . . . .  16
   Authors' Addresses  . . . . . . . . . . . . . . . . . . . . . . .  16

1.  Introduction

   TBD.

1.1.  Requirements Language

   The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
   "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
   document are to be interpreted as described in RFC 2119 [RFC2119].

2.  Common implementation mechanism

   In this section we describe some common implementation concepts.
   Basically, they follow from the preparation phase when different
   approaches were analysed.

   First of, the implementations are all done on modern Linux operating
   system, more specifically, Fedora Desktop distribution.  The main



Gros, et al.            Expires September 6, 2016               [Page 2]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   characteristic of this distribution is that the main IPC mechanism is
   D-Bus.  So, D-Bus is also used by the two implementations described
   in this draft.

   Next, in the analysis phase the implementations assessment was done
   to determine which mechanisms to use with the main goal of having
   control over the use of provisioning domains by applications.  In
   other words, the main issue was how to control which PvD will an
   application use in the presence of multiple provisioning domains.
   The problems we tried to address are well described in RFC 6418
   [RFC6418].  The conclusion was that the network namespaces are the
   most appropriate mechanism to control application use of provisioning
   domains.

   Still, while network namespaces were found to be the most appropriate
   machanism for separation there is a big issue in that it is not
   possible to have the same PvD instance in separate network
   namespaces.  To get around that issue multiple PvD instaces could be
   used by a single node.  Yet, this is not without the problems either.
   First, from the operational perspective this leads to proliferation
   of IP addresses and to issues with accountability.  It is even a
   bigger issue on IPv4 networks that have restricted number of PvD
   instances per each PvD.

3.  PvD-manager Implementation

   PvD-manager is a client-side component for IPv6 network auto-
   configuration based on multiple provisioning domains (PvDs), as
   described in RFC 6418 [RFC6418] and RFC 7556 [RFC7556].  PvD-manager
   is orthogonal to existing system, which means it does not interfere
   with regular network behavior: none of the services and settings used
   by Network Manager and similar components are not changed nor
   affected.  Implementation and documentation for PvD-manager is
   available at its git repository [PvD-manager].

3.1.  Architecture

   PvDs are implemented through Linux network namespaces.  For each
   coherent PvD information set received on the network interface, PvD-
   manager creates a separate network namespace and configures received
   network parameters within that namespace.  Since each network
   namespace uses separate IP stack which is isolated from other
   namespaces, potentially conflicting network parameters received from
   different network providers can safely coexist on a single host.
   PvD-manager manages only those newly created namespaces associated
   with the PvDs and their network settings and leaves the default
   network namespace intact.  That way, all the existing network




Gros, et al.            Expires September 6, 2016               [Page 3]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   management components, such as Network Manager, continue to work
   unobtrusively.

   Figure 1 presents an overview of a system and its components which
   uses PvD-manager.

 +---------------------------------------------------+  +-------------+
 | +-----------------------------------------------+ |  |   +-------+ |
 | | PvD-manager                                   | |  | +-+ radvd | |
 | | +-----------+   +-----------+   +-----------+ | |  | | +-------+ |
 | | | pvdserver +->-+   pvdman  +-<-+ ndpclient | | |  | |           |
 | | +-----+-----+   +-------+---+   +-----+-----+ | |  | | +-------+ |
 | +-------|-----------------|-------------|-------+ |  | +-+ httpd | |
 |         |d-bus      create|configure    |RA/RS    |  | | +-------+ |
 | get_pvds|           delete|             |HTTP     |  | |           |
 | +-------|-------+    +----+---------+   |         |  | |           |
 | | +-----+-----+ |join|   network    |  -+---------+--+-+--        -+-
 | | |  MIF API  +------+  namespace   |             |  |             |
 | | +-----------+ |    |  operations  |             |  |             |
 | |               |    +--------------+             |  |             |
 | |   PvD aware   |                                 |  |             |
 | |  application  |                                 |  |             |
 | +---------------+                    Client (PC)  |  |     Router  |
 +---------------------------------------------------+  +-------------+

               Figure 1: PvD prototype architecture overview

   PvD-manager receives network configurations through Router
   Advertisement (RA) messages.  Modified version of PvD aware radvd
   daemon [radvd] is used.  Each RA may contain one or more network
   configurations which are classified either as explicit or implicit
   PvDs.  Explicit PvD is a set of coherent NDP options explicitly
   labeled with a unique PvD identifier and nested within a special NDP
   option called PVD container, as described in draft-ietf-mif-mpvd-ndp-
   support-02 [I-D.ietf-mif-mpvd-ndp-support].  Multiple explicit PvDs
   may appear in a single RA, each within a different PvD container
   option, as long as they are labeled with different PvD identifiers.
   Implicit PvD is just another name for top-level NDP options placed
   outside the PvD container option, as in regular PvD unaware router
   advertisements.  Since implicit PvDs are not labeled with PvD
   identifier, PvD-manager automatically generates an identifier for
   internal use and configures the implicit PvD on the host in the same
   way as if it was the explicit one.  Only one implicit PvD is allowed
   per RA.  In current prototype, UUID is used as a PvD identifier.

   Each PvD, either explicit or implicit, is associated with a network
   namespace with a single virtual network interface (besides the
   loopback) of macvlan type, where PvD-related network parameters are



Gros, et al.            Expires September 6, 2016               [Page 4]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   configured.  To establish a connectivity to the outside world,
   virtual interface is connected to the physical interface on which the
   related PvD information is received through the RA.  Each virtual
   interface is assigned a link-local IPv6 address (fe80::/64) and one
   or more addresses derived from Prefix Information options if present
   in the received RA.  Besides the IP addresses, PvD-manager configures
   the routing tables and DNS records within the namespace.  By default,
   a link-local and default route via the RA-announcing router are added
   to the routing table, regardless of the routing information received
   in RA.  Additional routing information is configured if Route
   Information options are received in RA.  Finally, for each RDNSS and
   DNSSL option received in RA, PvD-manager creates a record in
   /etc/netns/NETNS_NAME/resolv.conf, where NETNS_NAME is a name of the
   network namespace associated with the PvD.

   PvD aware client application uses PvD API to get a list of available
   PvDs configured on the local host, and activate chosen PvD to use it
   for communication.  Information about configured PvDs are exposed to
   applications by a special PvD service running on local host.  D-Bus
   is used to connect applications to PvD service.  Upon PvD activation,
   client application is switched to the network namespace associated
   with the selected PvD.  Further network operations (socket creation,
   sending and receiving data) are performed within that namespace.
   Once obtained by the application, socket handles are linked to the
   network namespace they were originally obtained from and continue to
   work in that namespace, regardless of whether the application
   switches to another namespace at some time later.  This enables the
   application to use multiple PvDs simultaneously.  The only
   requirement is that the application is running within a proper
   network namespace while obtaining a socket.

   PvD unaware clients operate as before.  Although they are not able to
   use PvD API to select a certain PvD, they can still be forced to use
   a specific PvD by starting them in a network namespace associated
   with that PvD.  To run a program within a given namespace, it should
   be started with:

      ip netns exec <pvd-namespace-name> <command with args>

   or they can be started with provided launchers ("pvd_run" and
   "pvd_prop_run").

   As per RFC 7556 [RFC7556], implemented PvD system provides basic,
   intermediate and advanced PvD support (in APIs) for client
   applications.  Only difference is that our basic support doesn't
   provide automatic selection for PvD unaware application - it must be
   started with PvD launcher with manual selection of PvD.  Intermediate
   and advanced PvD support require some additional properties



Gros, et al.            Expires September 6, 2016               [Page 5]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   (metadata) provided with PvD.  Next section describe used mechanism
   to provide such information to PvD-manager which then provide such
   information to client applications.

3.2.  PvD Properties

   With RA messages routers provides network related parameters for
   PvDs.  Other parameters that can be used to detail properties of
   particular PvD (an application can use them to better select PvD) in
   this draft are called "PvD properties" or just "properties".

   In this prototype implementation, PvD properties are also provided by
   router, but only on request, using HTTP protocol on router's link-
   local address, using port 8080.  Router's link-local address is saved
   by PvD-manager when RA was received.

   Upon receiving PvD information from router, PvD-manager tries to get
   a file with PvD properties from the same router.  If such file
   exists, network related PvD parameters are extended with ones
   (properties) from received file.

   Client application receives all those additional properties from PvD-
   manager, and may select appropriate PvD based on them.

   Current implementation is very rudimentary: files on router are in
   JSON format.  PvD-manager interpret them - create a dictionary of
   them, but only because PvD-manager it's written in Python and using
   JSON is easy, while client applications are written in C.  In real
   implementation this should be reversed - only client should interpret
   file with PvDs' properties.

   Properties used in this prototype are just an example ("name",
   "type", "bandwidth", "price"), not to be used in some protocol
   specification.  We presented one mechanism to provide additional PvD
   properties obtained by some mechanism (not by RAs) and let client
   application decide what to do with them.

   Figure 2 presents example properties for several PvDs when obtained
   from two routers (R1 and R2 from test scenarios described in
   Section 3.5.











Gros, et al.            Expires September 6, 2016               [Page 6]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   From R1:
   [
       {
           "name": "Home internet access",
           "type": ["internet", "wired"],
           "id": "implicit",
           "bandwidth": "10 Mbps",
           "pricing": "free"
       },
       {
           "name": "TV",
           "type": ["iptv", "wired"],
           "id": "f037ea62-ee4f-44e4-825c-16f2f5cc9b3e",
           "bandwidth": "10 Mbps",
           "pricing": "free"
       }
   ]
   From R2:
   [
       {
           "name": "Cellular internet access",
           "type": ["internet", "cellular"],
           "id": "implicit",
           "bandwidth": "1 Mbps",
           "pricing": "0,01 $/MB"
       },
       {
           "name": "Phone",
           "type": ["voice", "cellular"],
           "id": "f037ea62-ee4f-44e4-825c-16f2f5cc9b3f",
           "bandwidth": "0,1 Mbps",
           "pricing": "0,01 $/MB"
       }
   ]

                      Figure 2: PvD property examples

3.3.  Deployment

   PvD architecture assumes presence of at least one router which runs
   modified version of radvd daemon [radvd], described in Section 5.
   Through RA messages, router conveys network related parameters to
   client host (prefix, routes, DNS servers and domains).  Router should
   also provide PvD properties, using an HTTP server on port 8080
   attached to router's link-local IP address.

   DNS servers aren't part of PvD architecture but could be used to
   demonstrate that different PvDs can use different DNS servers.



Gros, et al.            Expires September 6, 2016               [Page 7]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   PvD-manager is a daemon for client host.  Currently PvD-manager
   consists of several modules.  Main module maintain PvD information,
   creates, updates and deletes namespaces.  NDP module listens for RA
   messages, parse them and forward them to main module.  API server
   module listens for client application requests (using d-bus) and
   responds to them, and also send signals to clients when a change
   occurred in PvDs.

   Before starting a network connection, PvD aware client application
   should first request PvDs from PvD-manager.  Next, one PvD should be
   selected (activated) and only then network connection(s) created.  If
   other PvDs are required (later or in parallel), same procedure must
   be followed: select PvD first and then create connections.
   Connection will continue to operate within PvD in which was created,
   regardless of PvDs selected later.

   PvD unaware application should be started with PvD launcher to use
   certain PvD.  Otherwise, such application will behave as PvDs weren't
   present ("as usual").

3.4.  Implementation Details

   Proposed PvD architecture is based on Linux namespaces as PvD
   isolation mechanism.  Isolation namespaces provide resolve many
   issues about overlapping and conflicting network parameters for
   different PvDs.  However, they also impose some requirements that may
   limit usage in certain environment, especially ones based on public
   IPv4 addresses.  One of the main problem with namespaces is that each
   namespace requires an IP address (since namespace emulates network
   from link-layer and up).

   Only IPv6 is used in implementation.  Main reasons include using RA
   messages as PvD information provider and unrestricted generation of
   IPv6 address per PvD.

   A library is created with interfaces (API) for PvD operations,
   currently only for programs in C programming language.  For
   communication between client application (using provided API) and
   PvD-manager d-bus service is used.  Implemented interface include PvD
   retrieval methods (pvd_get_by_id, pvd_get_by_properties), PvD
   selection (pvd_activate) and registration for events when PvDs change
   (pvd_register_signal).  Sample test applications which demonstrate
   API usage and PvD system possibilities are provided in repository.

   PvD-manager is implemented in Python 3 because it allows rapid
   prototyping using network managing modules (mainly pyroute2 and
   netaddr).




Gros, et al.            Expires September 6, 2016               [Page 8]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   More details about implementation of PvD-manager is available in its
   documentation [PvD-manager].

3.5.  Test Scenarios

   Test scenarios used for validating implementations include a system
   with one client host, two routers and two hosts that act as servers,
   as presented on Figure 3.  All hosts are running as virtual machines.

               fd01::1/64+----+                                   +----+
         2001:db8:1::1/64|    |2001:db8:10::1/32 2001:db8:10::2/32|    |
            +----------o-+ R1 +-o-------------------------------o-+ S1 |
 +------+   |            |    |        :     [VMnet3]             |    |
 |      |   |            +----+        :                          +----+
 |Client+-o-+ [VMnet2]                 : (for some tests this is linked)
 |      |   |            +----+        :                          +----+
 +------+   |            |    |        :     [VMnet4]             |    |
            +----------o-+ R2 +-o-------------------------------o-+ S2 |
         2001:db8:2::1/64|    |2001:db8:20::1/32 2001:db8:20::2/32|    |
               fd02::1/64+----+                                   +----+

          Figure 3: Network configuration used in test scenarios

   All server hosts, including routers have configured HTTP and DNS
   servers providing many possibilities for testing.  Routers, in RAs
   advertise prefixes as shown on Figure 3.  Local addresses are used in
   explicit PvDs (simulating some specific service), while public in
   implicit.

   Example network configurations from RFC 7556 [RFC7556] are simulated
   with Figure 3 with various applications on Client and servers S1 and
   S2.  S1 is accessible by client only through implicit PvD provided by
   R1, while S2 similarly, only over PvD provided by R2.

   If S1 simulate one service, and S2 another, client application can
   select PvD based on service required and connect to S1 or S2.  Or a
   PvD aware application can use both in parallel.

   VPN was simulated by a tunnel between Client and S2, created within
   implicit PvD provided by R2.  Then tunnel was added as another PvD.
   S2 for this scenario had local address (and prefix) as local
   addresses on R2 network: there were two PvDs with same prefix on
   Client.  However, client applications running in those two different
   PvDs for the same IP address (fd02::1) connected with different
   servers: one with R2, and another (which used "VPN" PvD) with S2.

   In some scenarios both S1 and S2 were connected to both routers R1
   and R2.  In this scenarios better PvD could be chosen for connecting



Gros, et al.            Expires September 6, 2016               [Page 9]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   with servers, if properties for PvDs are provided.  Also, when some
   connection fails - when some PvD loses connectivity, application can
   reset connections, refresh PvD availability from PvD-manager and
   chose again among active PvDs.

   More details for described scenarios (and some other) are provided in
   demonstration test cases [PvD-manager].

3.6.  Experiences gained

   Present some experience gained from the implementation.  What do you
   think was good?  What would you do differently if you were to
   implement this again.  What about language/environment, was it good,
   what was bad?

3.6.1.  Linux namespaces

   Using Linux namespaces still seems best option for PvD realization
   despite its drawbacks.  Its isolation and ease of use from PvD-
   manager and client application perspective can hardly be outmatched
   with other solutions.  Besides already mentioned need for separate IP
   address per namespace, there are several more issues with namespaces.

   Managing namespaces (creation, deletion, modification) requires root
   privileges (as expected).  However, even switching an application
   from one namespace to another is possible only if application has
   root privileges.  This currently limits this namespace approach to
   only applications run by root.  For lifting this limitation, changes
   in Linux kernel and namespace handling is required.  Some sort of
   permission system should also be applied to namespaces (e.g.
   similarly with permissions on files and system objects).

   Switching namespace from within the application with setns doesn't
   update DNS related configuration as expected.  When an application is
   started with commands "ip netns exec <namespace-name> <application>
   [arguments]", DNS configuration is updated (/etc/resolf.conf is the
   one from /etc/netns/<namespace-name>/).  However, setns doesn't
   replicate that behavior, and that manipulations should be done
   separately (mounting certain directories/files).

   When a namespace is created, a virtual device is created that is
   linked with physical device (and it gets assigned its own MAC
   address).  However, if on particular physical device can't be linked
   virtual one (like VPN), then either physical device must be moved to
   certain PvD or some sort of bridge created and devices attached to
   that bridge that can be moved to namespace.  Sometimes, it's better
   to move physical device to particular namespace (PvD) and allow only
   some applications its usage (e.g. like VPN).



Gros, et al.            Expires September 6, 2016              [Page 10]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   Managing namespace: creating, deleting, adding device to it, adding
   ip address and routes are operations performed within PvD-manager and
   they aren't instant.  Maybe that is expected and "normal" since such
   operations aren't to be performed frequently (only when something in
   network changes).  However, when testing frequent changes in PvDs
   (routers were connected and disconnected) significant delay in PvD
   aware application was detected.  Sometimes PvD-manager's API server
   module (that is responsible for client communication) become
   unresponsive for at least several seconds.  This could be a bug in
   PvD-manager or result of changes PvD-manager sent being applied by
   Linux kernel.

   Recently added feature of Linux kernel (January, 2016), named Virtual
   Routing and Forwarding (VRF) [1], seems possible alternative to
   namespaces.  However, it should be investigate further to create some
   conclusions.

4.  NetworkManager implementation

   NetworkManager is a software component used in modern Linux
   distributions to control network connections.  It runs as a daemon
   tracking and reacting to the network related events, either those
   coming from the network (like, for example, Router Advertisements) or
   those from the local system (e.g. attachment of a new networking
   device).  Furthermore, it exposes certain methods and properties over
   D-Bus interface so that it can be controlled by different clients,
   the most prominent being network manager applet and nmcli command
   line tool.

   Due to its importance in the modern Linux distributions it was
   valuable experience to try to implement PvDs within it.  Yet,
   NetworkManager is a very complex piece of a software that wasn't
   designed with PvDs in mind so it wasn't straightforward task to try
   to implement them and there were some difficulties that had to be
   overcome.  In the following text we'll first describe how
   NetworkManager behaves now with respect to multiple PvDs, then we'll
   describe one approach on how to add support for multiple PvDs.

4.1.  NetworkManager's Current Behavior

   In this section we describe how unmodified NetworkManager behaves
   with respect to multiple provisioning domains.

   First, we have to state that NetworkManager doesn't use network
   namespaces, that is, everything is kept in one network namespace that
   we'll call "root network namespace" or "main network namespace".  So,
   all devices and all the configuration parameters are in one place.




Gros, et al.            Expires September 6, 2016              [Page 11]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   The situations in which NetworkManager handles multiple provisioning
   domains are:

   VPN Connections.
         When user activates VPN connection upon successful
         establishment of a connection, NetworkManager receives PvD
         instance for the given VPN connection.  In some cases after
         establishing VPN there is virtual device present (e.g.
         OpenVPN), while in other cases there are no new devices but
         only appropriate additions to the existing IP packet processing
         path (e.g.  IPsec).  In any case, interface and the associated
         PvD instance are assigned and present in the root network
         namespace.  Furthermore, DNS data is also merged with existing
         data.  It isn't hard to see that this mixing of PvD instances
         might lead to very complex situations in which it is hard to
         control which PvD instance will be used.  One possibility to
         control what will be accessible is default route.  Namely, it
         is possible to instruct NetworkManager not to install default
         route via VPN PvD instance even though it was sent by VPN
         gateway.

   Concurrent active wired and wireless connections.
         Again, like in the previous case, all the settings are mixed
         and this mixing has a consequence of not be able to use the two
         connections in parallel.  Current policy that is enforced by
         NetworkManager is to prefer wired connection for default route.
         In other words, unless there are more specific routes that use
         wireless connection, it will be not used while wired network
         connection is used.

   Multiple IPv6 routers on a local network.
         This is interesting use case even though it is not as common
         now.  Namely, there is a possibility that there are two IPv6
         capable routers on the local network that overlap in
         connectivity.  What NetworkManager will do in this case is that
         all PvDs received in RAs will be merged together per device.

   Multiple concurrent DNS servers on a local network.
         This is a final use case that isn't possible by the current
         protocol design.  Namely, DHCP offers received are treated as
         alternative and there is no way to have multiple DNS servers on
         a single local network.

4.2.  The architecture and components

   We can describe NetworkManager in terms of static and runtime
   architecture.  The static architecture is reflected by the source
   code organization.  For the purpose of this draft will only present



Gros, et al.            Expires September 6, 2016              [Page 12]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   NetworkManager itself, but take into account that there are
   additional components, too.  So, the NetworkManager's code is in src/
   directory of top-level NetworkManager directory.  There you'll find
   the following subdirectories:

   devices/
         Objects that control devices.

   dhcp-manager/


   dns-manager/


   dnsmasq-manager/


   platform/


   ppp-manager/


   rdisc/


   settings/


   supplicant-manager/


   systemd/


   vpn-manager/


4.3.  Implementation

   To understand NetworkManager it is first necessary to understand
   GObject system which is created so that it is possible to use object
   oriented programming in the C programming language, and also to allow
   easy integration with D-Bus.

   The implementation of PvD support within NetworkManager was done with
   the following requirements in mind:




Gros, et al.            Expires September 6, 2016              [Page 13]

Internet-Draft  An implementation of PvD support in Linux     March 2016


      For backwards compatibility there should be a root network
      namespace which is handled by NetworkManager as before, i.e.  it
      contains all configurations received merged into one.

      NetworkManager doesn't touch network namespaces created by other
      applications, like for example different virtualization solutions.

   The main objects that NetworkManager is based on are devices,
   connections and properties.

4.3.1.  Network namespace management implementation

   To support network namespace management it was necessary to add two
   objects: NMNetworkNamespaceController which is a singleton object
   that is used to enumerate available network namespaces, create a new
   one or remove existing network namespace.

   The second object introduced to allow NetworkManager support network
   namespaces is NMNetns.  For each network namespace created by
   NetworkManager one NMNetns object is created.  This object has an
   interface exposed over D-Bus that allows one to query all available
   devices within the network namespace, to take device from some other
   network namespace and to activate connections.

4.3.2.  Provisioning domain support implementation

5.  Server component

   Write about changes in the radvd component.

6.  Acknowledgements

   TBW.

7.  IANA Considerations

   This memo includes no request to IANA.

   All drafts are required to have an IANA considerations section (see
   the update of RFC 2434 [I-D.narten-iana-considerations-rfc2434bis]
   for a guide).  If the draft does not require IANA to do anything, the
   section contains an explicit statement that this is the case (as
   above).  If there are no requirements for IANA, the section will be
   removed during conversion into an RFC by the RFC Editor.







Gros, et al.            Expires September 6, 2016              [Page 14]

Internet-Draft  An implementation of PvD support in Linux     March 2016


8.  Security Considerations

   All drafts are required to have a security considerations section.
   See RFC 3552 [RFC3552] for a guide.

9.  References

9.1.  Normative References

   [RFC2119]  Bradner, S., "Key words for use in RFCs to Indicate
              Requirement Levels", BCP 14, RFC 2119,
              DOI 10.17487/RFC2119, March 1997,
              <http://www.rfc-editor.org/info/rfc2119>.

   [RFC6418]  Blanchet, M. and P. Seite, "Multiple Interfaces and
              Provisioning Domains Problem Statement", RFC 6418,
              DOI 10.17487/RFC6418, November 2011,
              <http://www.rfc-editor.org/info/rfc6418>.

   [RFC6419]  Wasserman, M. and P. Seite, "Current Practices for
              Multiple-Interface Hosts", RFC 6419, DOI 10.17487/RFC6419,
              November 2011, <http://www.rfc-editor.org/info/rfc6419>.

   [RFC7556]  Anipko, D., Ed., "Multiple Provisioning Domain
              Architecture", RFC 7556, DOI 10.17487/RFC7556, June 2015,
              <http://www.rfc-editor.org/info/rfc7556>.

9.2.  Informative References

   [I-D.ietf-mif-mpvd-dhcp-support]
              Krishnan, S., Korhonen, J., and S. Bhandari, "Support for
              multiple provisioning domains in DHCPv6", draft-ietf-mif-
              mpvd-dhcp-support-02 (work in progress), October 2015.

   [I-D.ietf-mif-mpvd-ndp-support]
              Korhonen, J., Krishnan, S., and S. Gundavelli, "Support
              for multiple provisioning domains in IPv6 Neighbor
              Discovery Protocol", draft-ietf-mif-mpvd-ndp-support-03
              (work in progress), February 2016.

   [I-D.narten-iana-considerations-rfc2434bis]
              Narten, T. and H. Alvestrand, "Guidelines for Writing an
              IANA Considerations Section in RFCs", draft-narten-iana-
              considerations-rfc2434bis-09 (work in progress), March
              2008.






Gros, et al.            Expires September 6, 2016              [Page 15]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   [RFC2629]  Rose, M., "Writing I-Ds and RFCs using XML", RFC 2629,
              DOI 10.17487/RFC2629, June 1999,
              <http://www.rfc-editor.org/info/rfc2629>.

   [RFC3552]  Rescorla, E. and B. Korver, "Guidelines for Writing RFC
              Text on Security Considerations", BCP 72, RFC 3552,
              DOI 10.17487/RFC3552, July 2003,
              <http://www.rfc-editor.org/info/rfc3552>.

9.3.  Implementation repositories

   [PvD-manager]
              Jelenkovic, L. and D. Skvorc, "PvD-manager repository",
              March 2016, <https://github.com/l30nard0/mif>.

   [radvd]    Skvorc, D., "PvD customized radvd daemon", February 2016,
              <https://github.com/dskvorc/mif-radvd>.

9.4.  URIs

   [1] https://www.kernel.org/doc/Documentation/networking/vrf.txt

Authors' Addresses

   Stjepan Gros (editor)
   University of Zagreb
   Unska 3
   Zagreb  10000
   HR

   Email: stjepan.gros@fer.hr


   Leonardo Jelenkovic
   University of Zagreb
   Unska 3
   Zagreb  10000
   HR

   Email: leonardo.jelenkovic@fer.hr











Gros, et al.            Expires September 6, 2016              [Page 16]

Internet-Draft  An implementation of PvD support in Linux     March 2016


   Dejan Skvorc
   University of Zagreb
   Unska 3
   Zagreb  10000
   HR

   Email: dejan.skvorc@fer.hr












































Gros, et al.            Expires September 6, 2016              [Page 17]
